{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from contextualized_topic_models.models.ctm import CTM\n",
    "from contextualized_topic_models.utils.data_preparation import TextHandler, bert_embeddings_from_file\n",
    "from contextualized_topic_models.datasets.dataset import CTMDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path=\"data/glassdoor/glassdoor_topics.parquet\"\n",
    "save_path=\"data/glassdoor/glassdoor_sentences.txt\"\n",
    "test_path=\"data/glassdoor/glassdoor_sentences_abridged.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_parquet(load_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt(save_path, df['text'].values, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt(test_path, df['text'].head(100000).values, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = TextHandler(test_path)\n",
    "handler.prepare() # create vocabulary and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate BERT data\n",
    "training_bert = bert_embeddings_from_file(test_path, \"bert-base-nli-mean-tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_encodings = \"data/glassdoor/bert_encodings.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(bert_encodings, training_bert)\n",
    "training_bert = np.load(bert_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = CTMDataset(handler.bow, training_bert, handler.idx2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training CTM\n",
    "ctm = CTM(\n",
    "    input_size=len(handler.vocab), \n",
    "    bert_input_size=768, \n",
    "    inference_type=\"combined\", \n",
    "    n_components=10,\n",
    "    num_data_loader_workers=1,\n",
    "    num_epochs=10,\n",
    "    use_gpu=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: \n",
      "               N Components: 50\n",
      "               Topic Prior Mean: 0.0\n",
      "               Topic Prior Variance: 0.98\n",
      "               Model Type: prodLDA\n",
      "               Hidden Sizes: (100, 100)\n",
      "               Activation: softplus\n",
      "               Dropout: 0.2\n",
      "               Learn Priors: True\n",
      "               Learning Rate: 0.002\n",
      "               Momentum: 0.99\n",
      "               Reduce On Plateau: False\n",
      "               Save Dir: models/CTM/01\n",
      "Epoch: [1/10]\tSamples: [100637/1006370]\tTrain Loss: 90.95055373976753\tTime: 0:01:20.846262\n",
      "Epoch: [2/10]\tSamples: [201274/1006370]\tTrain Loss: 88.00021321700301\tTime: 0:01:21.818298\n",
      "Epoch: [3/10]\tSamples: [301911/1006370]\tTrain Loss: 87.3909494954639\tTime: 0:01:21.253848\n",
      "Epoch: [4/10]\tSamples: [402548/1006370]\tTrain Loss: 87.04632548769806\tTime: 0:01:20.516218\n",
      "Epoch: [5/10]\tSamples: [503185/1006370]\tTrain Loss: 86.8078568085747\tTime: 0:01:20.250964\n",
      "Epoch: [6/10]\tSamples: [603822/1006370]\tTrain Loss: 86.62441147594899\tTime: 0:01:20.467594\n",
      "Epoch: [7/10]\tSamples: [704459/1006370]\tTrain Loss: 86.56132923511606\tTime: 0:01:20.457595\n",
      "Epoch: [8/10]\tSamples: [805096/1006370]\tTrain Loss: 86.5255846284558\tTime: 0:01:20.551293\n",
      "Epoch: [9/10]\tSamples: [905733/1006370]\tTrain Loss: 86.43954225451064\tTime: 0:01:21.089833\n",
      "Epoch: [10/10]\tSamples: [1006370/1006370]\tTrain Loss: 86.38484296923131\tTime: 0:01:21.752325\n"
     ]
    }
   ],
   "source": [
    "ctm.fit(training_dataset, save_dir=\"models/CTM/01\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['years', 'years.', '3', 'months', '2'],\n",
       " ['joke.', 'pieces', 'Pay.', 'lie.', 'parking.'],\n",
       " ['need', 'make', 'actually', 'employees', 'hard'],\n",
       " [\"I'm\", 'worst', 'wish', 'for.', 'there.'],\n",
       " ['help', 'culture', 'willing', 'truly', 'clients'],\n",
       " ['doing.', 'Do', 'doing!', 'Keep', \"Don't\"],\n",
       " ['place', 'It', 'This', 'experience', 'lot'],\n",
       " ['growing', 'learn', 'Lots', 'opportunity', 'Company'],\n",
       " ['Get', 'up!', 'Be', 'Keep', 'doing!'],\n",
       " ['benefits,', 'pay,', 'life', 'good', 'free']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the topics\n",
    "ctm.get_topic_lists(5)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
