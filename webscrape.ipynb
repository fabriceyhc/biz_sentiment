{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = pd.read_csv(\"data/brands/tickers/tickers.csv\")['tic'].dropna()\n",
    "tickers = tickers.str.replace('\\d+', '')\n",
    "tickers = tickers.str.split('.').apply(lambda x: x[0])\n",
    "tickers = set(tickers.values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Selenium / BeautifulSoup Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# executable_path = 'C:/Users/Fabrice/Documents/GitHub/biz_sentiment/webscrape/drivers/chromedriver.exe'\n",
    "# driver = webdriver.Chrome(executable_path=executable_path)\n",
    "\n",
    "# ticker = 'VVUSQ'\n",
    "# skip_inactive = False\n",
    "# pagetypes = ['competitor', 'customer', 'supplier', 'partner']\n",
    "\n",
    "# base_url = \"https://www.mergentonline.com/\"\n",
    "# search_url = base_url + \"basicsearch.php\"\n",
    "\n",
    "# driver.get(search_url)\n",
    "\n",
    "# searchbox = driver.find_element_by_id(\"basicsearchinput\")\n",
    "# searchbox.clear()\n",
    "# searchbox.send_keys(ticker)\n",
    "\n",
    "# wait = WebDriverWait(driver, 5)\n",
    "# wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'livesearchresult')))\n",
    "\n",
    "# soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "# table = soup.find('table', class_='livesearchresult')\n",
    "\n",
    "# df_table = pd.read_html(str(table))[0]\n",
    "# df_table.columns = ['Ticker', 'Exchange', 'Name']\n",
    "\n",
    "# us_exchanges = ['BZX', 'NAS','CHX', 'IEX', 'ISE', 'NMS', 'NYS']\n",
    "# df_table_us = df_table[df_table['Exchange'].isin(us_exchanges)]\n",
    "# df_table_us = df_table_us[df_table_us['Ticker'] == ticker]\n",
    "# if skip_inactive: \n",
    "#     df_table_us = df_table_us[~df_table_us['Name'].str.contains('inactive')]\n",
    "# if len(df_table_us) > 0:\n",
    "#     idx = df_table_us.index[0]\n",
    "\n",
    "# redirected=False\n",
    "# supply_chain_url = base_url + 'companyhorizon.php?'\n",
    "# compnumber = table.find_all('a')[idx].get('href').split('&')[0].split('?')[1]\n",
    "# page = supply_chain_url + compnumber\n",
    "# driver.get(page)\n",
    "# redirected=True\n",
    "\n",
    "# downloaded=False\n",
    "# for pagetype in pagetypes:\n",
    "#     tab = supply_chain_url + 'pagetype=' + pagetype + '&' + compnumber\n",
    "#     driver.get(tab)\n",
    "#     if 'no ' + pagetype + ' found' in driver.page_source.lower():\n",
    "#         continue\n",
    "#     driver.find_element_by_id(\"excellinkid\").click()\n",
    "#     downloaded = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergent_supply_chain_search(driver, ticker):\n",
    "    \n",
    "    base_url = \"https://www.mergentonline.com/\"\n",
    "    search_url = base_url + \"horizonsearch.php\"\n",
    "\n",
    "    driver.get(search_url)\n",
    "\n",
    "    searchbox = driver.find_element_by_id(\"searchtext\")\n",
    "    searchbox.clear()\n",
    "    searchbox.send_keys(ticker)\n",
    "\n",
    "    wait = WebDriverWait(driver, 5)\n",
    "    wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'livesearchresult')))\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    results = soup.find('table', class_='livesearchresult')\n",
    "    if results is None:\n",
    "        return False\n",
    "    links = results.find_all('a')\n",
    "    redirected=False\n",
    "    for link in links:\n",
    "        if '-US%29' in str(link):\n",
    "            driver.get(base_url + link.get('href'))\n",
    "            redirected=True\n",
    "    if not redirected:\n",
    "        # print('Could not find US company for stock ticker:', ticker)\n",
    "        return False\n",
    "\n",
    "    driver.find_element_by_id(\"excel\").click() \n",
    "    return True\n",
    "\n",
    "def mergent_basic_search(driver, ticker, pagetypes = ['competitor', 'customer', 'supplier', 'partner'], skip_inactive=True):\n",
    "    \n",
    "    base_url = \"https://www.mergentonline.com/\"\n",
    "    search_url = base_url + \"basicsearch.php\"\n",
    "\n",
    "    driver.get(search_url)\n",
    "\n",
    "    searchbox = driver.find_element_by_id(\"basicsearchinput\")\n",
    "    searchbox.clear()\n",
    "    searchbox.send_keys(ticker)\n",
    "\n",
    "    wait = WebDriverWait(driver, 5)\n",
    "    wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'livesearchresult')))\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    table = soup.find('table', class_='livesearchresult')\n",
    "\n",
    "    df_table = pd.read_html(str(table))[0]\n",
    "    df_table.columns = ['Ticker', 'Exchange', 'Name']\n",
    "\n",
    "    us_exchanges = ['BZX', 'NAS','CHX', 'IEX', 'ISE', 'NMS', 'NYS']\n",
    "    df_table_us = df_table[df_table['Exchange'].isin(us_exchanges)]\n",
    "    df_table_us = df_table_us[df_table_us['Ticker'] == ticker]\n",
    "    if skip_inactive: \n",
    "        df_table_us = df_table_us[~df_table_us['Name'].str.contains('inactive')]\n",
    "    if len(df_table_us) > 0:\n",
    "        idx = df_table_us.index[0]\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    redirected=False\n",
    "    supply_chain_url = base_url + 'companyhorizon.php?'\n",
    "    compnumber = table.find_all('a')[idx].get('href').split('&')[0].split('?')[1]\n",
    "    page = supply_chain_url + compnumber\n",
    "    driver.get(page)\n",
    "    redirected=True\n",
    "\n",
    "    if not redirected:\n",
    "        # print('Could not find US company for stock ticker:', ticker)\n",
    "        return False\n",
    "    \n",
    "    downloaded=False\n",
    "    for pagetype in pagetypes:\n",
    "        tab = supply_chain_url + 'pagetype=' + pagetype + '&' + compnumber\n",
    "        driver.get(tab)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        if not soup.find('table', class_='tablesorter bodyline'):\n",
    "            continue\n",
    "        driver.find_element_by_id(\"excellinkid\").click()\n",
    "        downloaded = True\n",
    "        \n",
    "    if not downloaded:\n",
    "        return False \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = 'C:/Users/Fabrice/Documents/GitHub/biz_sentiment/webscrape/drivers/chromedriver.exe'\n",
    "driver = webdriver.Chrome(executable_path=executable_path)\n",
    "\n",
    "tickers = set(pd.read_csv(\"data/tickers/tickers.csv\")['tic'].str.replace('[^a-zA-Z]', '').values.tolist())\n",
    "tickers = [ticker for ticker in tickers if isinstance(ticker, str)]\n",
    "\n",
    "print('Total tickers:', len(tickers))\n",
    "\n",
    "passes, fails = [], []\n",
    "for ticker in tickers:\n",
    "    success = mergent_supply_chain_search(driver, ticker)\n",
    "    if success:\n",
    "        passes.append(ticker)\n",
    "    else:\n",
    "        fails.append(ticker)\n",
    "        \n",
    "print('Total passes:', len(passes))\n",
    "print('Total fails:', len(fails))\n",
    "    \n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total passes: 9097\n",
      "Total fails: 7374\n"
     ]
    }
   ],
   "source": [
    "print('Total passes:', len(passes))\n",
    "print('Total fails:', len(fails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "passes, fails = list(set(passes)), list(set(fails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-126-56e225e5684b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mticker\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfails\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0msuccess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdownload_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mticker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mfails\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-3c74444aeeee>\u001b[0m in \u001b[0;36mdownload_excel\u001b[1;34m(driver, ticker)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0msearchbox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "executable_path = 'C:/Users/Fabrice/Documents/GitHub/biz_sentiment/webscrape/drivers/chromedriver.exe'\n",
    "driver = webdriver.Chrome(executable_path=executable_path)\n",
    "\n",
    "for _ in range(2):\n",
    "    for ticker in fails:\n",
    "        success = mergent_supply_chain_search(driver, ticker)\n",
    "        if success:\n",
    "            fails.remove(ticker)\n",
    "            passes.append(ticker)\n",
    "        else:\n",
    "            fails.append(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total passes: 9798\n",
      "Total fails: 6667\n"
     ]
    }
   ],
   "source": [
    "print('Total passes:', len(passes))\n",
    "print('Total fails:', len(fails))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining all excel files together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = glob.glob(r'C:\\Users\\Fabrice\\Downloads\\searchresult*')\n",
    "target_dir = r\"C:\\Users\\Fabrice\\Documents\\GitHub\\biz_sentiment\\data\\brands\\mergent_excels\"\n",
    "for path in paths:\n",
    "    file_name = os.path.basename(path)\n",
    "    new_path = os.path.join(target_dir, file_name)\n",
    "    shutil.move(path, new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_paths = glob.glob(os.path.join(target_dir, \"searchresult*\"))\n",
    "df = pd.concat((pd.read_html(f)[0] for f in new_paths))\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "df.rename(columns={'index': 'brand_num'}, inplace=True)\n",
    "df['Brands'] = df['Tradename'].str.split(' - ').apply(lambda x: '' if len(x) == 1 else x[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/brands/brands_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(keyword):\n",
    "    \n",
    "    # find all files in Downloads and move them to the project dir\n",
    "    paths = glob.glob(\"C:/Users/Fabrice/Downloads/\" + keyword + \"*\")\n",
    "    target_dir = os.path.join(\"C:/Users/Fabrice/Documents/GitHub/biz_sentiment/data/brands\", keyword)\n",
    "    os.makedirs(target_dir, exist_ok = True) \n",
    "    for path in paths:\n",
    "        file_name = os.path.basename(path)\n",
    "        new_path = os.path.join(target_dir, file_name)\n",
    "        shutil.move(path, new_path)\n",
    "        \n",
    "    # load as df    \n",
    "    new_paths = glob.glob(os.path.join(target_dir, keyword + \"*\"))\n",
    "    df = pd.concat((pd.read_html(f)[0] for f in new_paths))\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    # save as csv\n",
    "    df.to_csv(os.path.join(target_dir, keyword + '.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "for keyword in ['supplier', 'customer', 'competitor', 'partner']:\n",
    "    preprocess(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'customer'\n",
    "target_dir = os.path.join(\"C:/Users/Fabrice/Documents/GitHub/biz_sentiment/data/brands\", keyword)\n",
    "new_paths = glob.glob(os.path.join(target_dir, keyword + \"*\"))\n",
    "# df = pd.concat((pd.read_html(f)[0] for f in new_paths))\n",
    "# df.drop_duplicates(inplace=True)\n",
    "# df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer\n",
      "competitor\n",
      "partner\n"
     ]
    }
   ],
   "source": [
    "for keyword in ['supplier', 'customer', 'competitor', 'partner']:\n",
    "    \n",
    "    print(keyword)\n",
    "\n",
    "    target_dir = os.path.join(\"C:/Users/Fabrice/Documents/GitHub/biz_sentiment/data/brands\", keyword)\n",
    "    new_paths = glob.glob(os.path.join(target_dir, keyword + \"*.xls\"))\n",
    "\n",
    "    # create df\n",
    "    df = None\n",
    "    for f in new_paths:\n",
    "        with open(f, encoding=\"utf8\") as fp:\n",
    "            soup = BeautifulSoup(fp)\n",
    "            title = soup.h2.text\n",
    "        if df is None:\n",
    "            df = pd.read_html(f)[0]\n",
    "            df['company'] = title\n",
    "        else:\n",
    "            next_df = pd.read_html(f)[0]\n",
    "            next_df['company'] = title\n",
    "            df = df.append(next_df)\n",
    "    # preprocessing\n",
    "    df['company'] = df['company'].str.replace(' \\(New\\) ', ' ')\n",
    "    df['exchange'] = df['company'].apply(lambda x: re.search('\\w+:', x).group(0)[:-1])\n",
    "    df['ticker'] = df['company'].apply(lambda x: re.search(': \\w+', x).group(0)[2:])\n",
    "    df['company'] = df['company'].str.split(' \\(\\w+: \\w+\\)').apply(lambda x:x[0])\n",
    "    df.loc[df['Dir'] == '<', 'Dir'] = 'Others'\n",
    "    df.loc[df['Dir'] == '>', 'Dir'] = 'Company'\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename(columns={'Dir': 'defined_by',\n",
    "                       'index': keyword + '_num', \n",
    "                       'Company': keyword + '_company',\n",
    "                       'Ticker': keyword + '_ticker'}, inplace=True)\n",
    "    cols = list(df.columns)\n",
    "    cols = cols[-3:] + cols[:-3]\n",
    "    df = df[cols]\n",
    "    # save to csv\n",
    "    df.to_csv(os.path.join(target_dir, '..', keyword + '.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3166"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['company'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spotchecking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.google.com/spreadsheets/d/12W0NATAeD8twLSGR3S8kYkUUtsVPCGlE443pqw05Nwo/edit#gid=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SWIM',\n",
       " 'TTDKY',\n",
       " 'CMSB',\n",
       " 'SPPR',\n",
       " 'CNVAF',\n",
       " 'DAVD',\n",
       " 'BHIX',\n",
       " 'GAMEE',\n",
       " 'TRDFF',\n",
       " 'LBTF',\n",
       " 'VBTX',\n",
       " 'STMP',\n",
       " 'SEGP',\n",
       " 'CEG',\n",
       " 'PA',\n",
       " 'EVVV',\n",
       " 'SLR',\n",
       " 'BWLK',\n",
       " 'UOUT',\n",
       " 'AMSI']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(fails, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Competitor, Customer, Supplier, Partner data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = 'C:/Users/Fabrice/Documents/GitHub/biz_sentiment/webscrape/drivers/chromedriver.exe'\n",
    "driver = webdriver.Chrome(executable_path=executable_path)\n",
    "\n",
    "print('Total tickers:', len(tickers))\n",
    "\n",
    "found, unfound = set(), tickers\n",
    "for _ in range(3):\n",
    "    for ticker in list(unfound):\n",
    "        success = mergent_basic_search(driver, ticker, skip_inactive=False)\n",
    "        if success:\n",
    "            found.union(ticker)\n",
    "        else:\n",
    "            unfound.union(ticker)\n",
    "\n",
    "print('Total found:', len(found))\n",
    "print('Total unfound:', len(unfound))\n",
    "    \n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver = webdriver.Chrome(executable_path=executable_path)\n",
    "# mergent_basic_search(driver, 'BHP', skip_inactive=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Selenium Approach (Unstable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selenium.webdriver.common.keys import Keys\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.support.wait import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def download_excel(ticker):\n",
    "#     driver.get(\"https://www.mergentonline.com/horizonsearch.php\")\n",
    "\n",
    "#     searchbox = driver.find_element_by_id(\"searchtext\")\n",
    "#     searchbox.clear()\n",
    "#     searchbox.send_keys(ticker)\n",
    "\n",
    "\n",
    "#     search_table = WebDriverWait(driver, 3).until(\n",
    "#         EC.presence_of_element_located(\n",
    "#             (By.CLASS_NAME, \"livesearchresult\")\n",
    "#         )\n",
    "#     ) \n",
    "#     rows = search_table.find_elements(By.TAG_NAME, \"tr\")\n",
    "#     for row in rows:\n",
    "#         driver.implicitly_wait(5)\n",
    "#         cols = row.find_elements(By.TAG_NAME, \"td\")\n",
    "#         try:\n",
    "#             driver.implicitly_wait(5)\n",
    "#             link = cols[0].find_elements(By.TAG_NAME, \"a\")[0].get_attribute(\"href\")\n",
    "#             driver.implicitly_wait(5)\n",
    "#             ticker = cols[1].text\n",
    "#             # print(link, ticker)\n",
    "#             if 'US' in ticker[-2:]:\n",
    "#                 driver.get(link)\n",
    "#         except:\n",
    "#             driver.implicitly_wait(5)\n",
    "#             link = cols[0].find_elements(By.TAG_NAME, \"a\")[0].get_attribute(\"href\")\n",
    "#             driver.implicitly_wait(5)\n",
    "#             ticker = cols[1].text\n",
    "#             # print(link, ticker)\n",
    "#             if 'US' in ticker[-2:]:\n",
    "#                 driver.get(link)\n",
    "\n",
    "#     download_excel = driver.find_element_by_id(\"excel\")\n",
    "#     download_excel.click()\n",
    "    \n",
    "#     driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
